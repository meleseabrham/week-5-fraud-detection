{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Credit Card Fraud Detection - Complete EDA\n",
                "## Task 1: EDA and Preprocessing for CreditCard Dataset\n",
                "\n",
                "This notebook analyzes the creditcard.csv dataset with:\n",
                "- Comprehensive EDA\n",
                "- Missing value analysis\n",
                "- Feature scaling\n",
                "- Class imbalance handling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.preprocessing import RobustScaler\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set style\n",
                "sns.set_style('whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "\n",
                "print('Libraries imported successfully!')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load data\n",
                "DATA_PATH = '../data/raw/creditcard.csv'\n",
                "df = pd.read_csv(DATA_PATH)\n",
                "\n",
                "print(f'✓ Data loaded successfully!')\n",
                "print(f'Shape: {df.shape}')\n",
                "print(f'\\nFirst few rows:')\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Cleaning\n",
                "### 2.1 Missing Values"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check for missing values\n",
                "print('Missing Values Analysis:')\n",
                "print('='*50)\n",
                "missing = df.isnull().sum()\n",
                "print(missing[missing > 0])\n",
                "\n",
                "if missing.sum() == 0:\n",
                "    print('\\n✓ No missing values found!')\n",
                "else:\n",
                " print(f'\\n⚠ Total missing: {missing.sum()}')\n",
                "\n",
                "# Data types\n",
                "print('\\nData Types:')\n",
                "print(df.dtypes.value_counts())\n",
                "\n",
                "# Basic statistics\n",
                "print('\\nBasic Info:')\n",
                "print(df.info())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 Remove Duplicates"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check and remove duplicates\n",
                "initial_rows = len(df)\n",
                "df.drop_duplicates(inplace=True)\n",
                "duplicates_removed = initial_rows - len(df)\n",
                "\n",
                "print(f'Duplicates removed: {duplicates_removed}')\n",
                "print(f'Final shape: {df.shape}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Exploratory Data Analysis\n",
                "### 3.1 Class Distribution"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Class distribution\n",
                "class_dist = df['Class'].value_counts()\n",
                "class_pct = df['Class'].value_counts(normalize=True) * 100\n",
                "\n",
                "print('Class Distribution:')\n",
                "print('='*50)\n",
                "print(f'Normal (0): {class_dist[0]:,} ({class_pct[0]:.4f}%)')\n",
                "print(f'Fraud (1): {class_dist[1]:,} ({class_pct[1]:.4f}%)')\n",
                "print(f'\\n⚠ Imbalance Ratio: {class_dist[0]/class_dist[1]:.0f}:1')\n",
                "print('This is a HIGHLY imbalanced dataset!')\n",
                "\n",
                "# Visualize\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Count plot\n",
                "sns.countplot(data=df, x='Class', ax=ax1)\n",
                "ax1.set_title('Class Distribution', fontsize=14, fontweight='bold')\n",
                "ax1.set_xticklabels(['Normal', 'Fraud'])\n",
                "for p in ax1.patches:\n",
                "    ax1.annotate(f'{int(p.get_height()):,}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
                "                ha='center', va='bottom')\n",
                "\n",
                "# Pie chart (log scale for visibility)\n",
                "ax2.pie(class_dist, labels=['Normal', 'Fraud'], autopct='%1.4f%%', startangle=90)\n",
                "ax2.set_title('Class Percentage', fontsize=14, fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.2 Time and Amount Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Time and Amount statistics\n",
                "print('Time and Amount Statistics:')\n",
                "print('='*50)\n",
                "print('\\nTime (seconds from first transaction):')\n",
                "print(df['Time'].describe())\n",
                "print('\\nAmount:')\n",
                "print(df['Amount'].describe())\n",
                "\n",
                "# Visualize\n",
                "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
                "\n",
                "# Time distribution\n",
                "axes[0, 0].hist(df['Time'], bins=50, edgecolor='black')\n",
                "axes[0, 0].set_title('Time Distribution', fontsize=12, fontweight='bold')\n",
                "axes[0, 0].set_xlabel('Time (seconds)')\n",
                "axes[0, 0].set_ylabel('Frequency')\n",
                "\n",
                "# Amount distribution (log scale)\n",
                "axes[0, 1].hist(df[df['Amount'] > 0]['Amount'], bins=50, edgecolor='black')\n",
                "axes[0, 1].set_yscale('log')\n",
                "axes[0, 1].set_title('Amount Distribution (log scale)', fontsize=12, fontweight='bold')\n",
                "axes[0, 1].set_xlabel('Amount')\n",
                "\n",
                "# Amount by class\n",
                "df.boxplot(column='Amount', by='Class', ax=axes[1, 0])\n",
                "axes[1, 0].set_title('Amount by Class', fontsize=12, fontweight='bold')\n",
                "axes[1, 0].set_xticklabels(['Normal', 'Fraud'])\n",
                "\n",
                "# Time by class\n",
                "df.boxplot(column='Time', by='Class', ax=axes[1, 1])\n",
                "axes[1, 1].set_title('Time by Class', fontsize=12, fontweight='bold')\n",
                "axes[1, 1].set_xticklabels(['Normal', 'Fraud'])\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.3 PCA Feature Analysis\n",
                "\n",
                "Note: V1-V28 are PCA-transformed features for confidentiality"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze PCA features\n",
                "v_features = [f'V{i}' for i in range(1, 29)]\n",
                "\n",
                "# Show a subset of V features distributions\n",
                "fig, axes = plt.subplots(3, 3, figsize=(18, 12))\n",
                "axes = axes.ravel()\n",
                "\n",
                "for idx, feature in enumerate(['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9']):\n",
                "    # Plot distribution by class\n",
                "    df[df['Class'] == 0][feature].hist(bins=50, alpha=0.5, label='Normal', ax=axes[idx])\n",
                "    df[df['Class'] == 1][feature].hist(bins=50, alpha=0.5, label='Fraud', ax=axes[idx])\n",
                "    axes[idx].set_title(f'{feature} Distribution by Class', fontsize=10)\n",
                "    axes[idx].legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print('\\nObservation: Some PCA features show clear separation between fraud and normal transactions')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.4 Correlation Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Correlation with target\n",
                "correlations = df.corr()['Class'].sort_values(ascending=False)\n",
                "print('Top 10 features correlated with fraud:')\n",
                "print(correlations.head(11))  # 11 to include Class itself\n",
                "\n",
                "print('\\nBottom 10 features (negative correlation):')\n",
                "print(correlations.tail(10))\n",
                "\n",
                "# Visualize top correlations\n",
                "fig, ax = plt.subplots(figsize=(10, 8))\n",
                "top_features = correlations.abs().sort_values(ascending=False)[1:16]  # Top 15 excluding Class\n",
                "top_features.plot(kind='barh', ax=ax)\n",
                "ax.set_title('Top 15 Features by Absolute Correlation with Fraud', fontsize=14, fontweight='bold')\n",
                "ax.set_xlabel('Absolute Correlation')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Feature Scaling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Scale Time and Amount using RobustScaler (less sensitive to outliers)\n",
                "print('Scaling Amount and Time features...')\n",
                "\n",
                "rob_scaler = RobustScaler()\n",
                "\n",
                "df['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1, 1))\n",
                "df['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1, 1))\n",
                "\n",
                "# Drop original features\n",
                "df.drop(['Time', 'Amount'], axis=1, inplace=True)\n",
                "\n",
                "# Reorder columns\n",
                "scaled_amount = df['scaled_amount']\n",
                "scaled_time = df['scaled_time']\n",
                "df.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\n",
                "df.insert(0, 'scaled_amount', scaled_amount)\n",
                "df.insert(1, 'scaled_time', scaled_time)\n",
                "\n",
                "print('✓ Features scaled successfully!')\n",
                "print(f'New shape: {df.shape}')\n",
                "print('\\nFirst few rows after scaling:')\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Final Dataset Overview"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Final summary\n",
                "print('FINAL DATASET SUMMARY')\n",
                "print('='*50)\n",
                "print(f'Total transactions: {len(df):,}')\n",
                "print(f'Total features: {df.shape[1]}')\n",
                "print(f'\\nClass Distribution:')\n",
                "print(df['Class'].value_counts())\n",
                "print(f'\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB')\n",
                "\n",
                "print('\\n✓ Dataset ready for modeling!')\n",
                "print('\\nNote: This dataset is PCA-transformed and highly imbalanced.')\n",
                "print('SMOTE or other resampling techniques recommended before training.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Save Processed Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save processed data\n",
                "import os\n",
                "PROCESSED_DIR = '../data/processed'\n",
                "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
                "\n",
                "df.to_csv(f'{PROCESSED_DIR}/creditcard_processed.csv', index=False)\n",
                "print(f'✓ Processed data saved to: {PROCESSED_DIR}/creditcard_processed.csv')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "### CreditCard Dataset Analysis Complete:\n",
                "\n",
                "✅ **Data Cleaning**\n",
                "- No missing values\n",
                "- Duplicates removed\n",
                "- Data types verified\n",
                "\n",
                "✅ **EDA Completed**\n",
                "- Extreme class imbalance identified (99.83% vs 0.17%)\n",
                "- PCA features analyzed\n",
                "- Correlation with fraud identified\n",
                "-Time and Amount patterns explored\n",
                "\n",
                "✅ **Feature Engineering**\n",
                "- RobustScaler applied to Time and Amount\n",
                "- Features ready for modeling\n",
                "\n",
                "### Key Findings:\n",
                "1. **Severe Class Imbalance**: Only 0.17% fraud - requires SMOTE/resampling\n",
                "2. **PCA Features**: Already dimensionality-reduced for privacy\n",
                "3. **Important Features**: V17, V14, V12, V10 show strong correlation with fraud\n",
                "4. **Amount**: Fraud transactions tend to have different amount patterns\n",
                "\n",
                "### Next Steps:\n",
                "- Apply SMOTE before training\n",
                "- Train classification models\n",
                "- Focus on Recall and F1-score metrics"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}