
# EDA and Feature Engineering for Fraud Data

import sys
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from imblearn.over_sampling import SMOTE
from collections import Counter

# Define paths
DATA_DIR = '../data/raw'
FRAUD_DATA_PATH = os.path.join(DATA_DIR, 'Fraud_Data.csv')
IP_COUNTRY_PATH = os.path.join(DATA_DIR, 'IpAddress_to_Country.csv')

# 1. Load Data
try:
    fraud_df = pd.read_csv(FRAUD_DATA_PATH)
    ip_country_df = pd.read_csv(IP_COUNTRY_PATH)
    print("Datasets loaded successfully.")
    
    # 2. Data Cleaning
    # Remove duplicates
    fraud_df.drop_duplicates(inplace=True)
    
    # Dates
    fraud_df['signup_time'
] = pd.to_datetime(fraud_df['signup_time'
])
    fraud_df['purchase_time'
] = pd.to_datetime(fraud_df['purchase_time'
])
    
    print(f"Fraud Data Shape: {fraud_df.shape}")
    print(fraud_df.info())
    
except FileNotFoundError:
    print("Ensure Fraud_Data.csv and IpAddress_to_Country.csv are in data/raw/")
    # Halt execution logic in notebook context if data missing

# 3. Geolocation Integration 
if 'fraud_df' in locals() and not fraud_df.empty:
    print("Mapping IP to Country...")
    
    # Optimize IP Mapping
    # Convert IP to numeric if needed, usually comes as float/int in CSV but let's ensure
    # Note: The problem might state IP is string or int.
    # Usually it's float/int in this specific dataset. 
    # Just in case generic safe conversion:
    # fraud_df['ip_address'
] = fraud_df['ip_address'
].astype(float).astype(int)
    
    # Using sort merge approach
    ip_country_df = ip_country_df.sort_values('lower_bound_ip_address')
    fraud_df_sorted = fraud_df.sort_values('ip_address')
    
    merged = pd.merge_asof(
        fraud_df_sorted,
        ip_country_df,
        left_on='ip_address',
        right_on='lower_bound_ip_address'
    )
    
    # Filter valid ranges
    merged['country'
] = np.where(merged['ip_address'
] <= merged['upper_bound_ip_address'
], merged['country'
], 'Unknown')
    
    # Restore original index/order if needed, or just proceed with merged
    fraud_df = merged
    
    print("Top Countries with Fraud:")
    print(fraud_df[fraud_df['class'
    ] == 1
]['country'
].value_counts().head())

# 4. Feature Engineering
if 'fraud_df' in locals() and not fraud_df.empty:
    print("Engineering features...")
    
    # Transaction velocity (by device)
    fraud_df['device_txn_count'
] = fraud_df.groupby('device_id')['device_id'
].transform('count')
    # Use ip counts
    fraud_df['ip_txn_count'
] = fraud_df.groupby('ip_address')['ip_address'
].transform('count')

    # Time-based
    fraud_df['hour_of_day'
] = fraud_df['purchase_time'
].dt.hour
    fraud_df['day_of_week'
] = fraud_df['purchase_time'
].dt.dayofweek
    fraud_df['time_since_signup'
] = (fraud_df['purchase_time'
] - fraud_df['signup_time'
]).dt.total_seconds() / 3600 # hours
    
    print("Features added.")

# 5. Transformation
if 'fraud_df' in locals() and not fraud_df.empty:
    print("Transforming data...")
    
    drop_cols = ['user_id', 'signup_time', 'purchase_time', 'device_id', 'ip_address', 
                 'lower_bound_ip_address', 'upper_bound_ip_address', 'country_x', 'country_y'
] 
                 # country_x/y might result from merge, usually just one 'country' if clean
                 # Adjust based on merge output. Merge_asof keeps both if names differ, or overlapping.
                 # The 'country' column we created is usually sufficient.
    
    # Clean up columns from merge
    cols_to_drop = [c for c in drop_cols if c in fraud_df.columns
]
    
    # Keep target
    y = fraud_df['class'
]
    X = fraud_df.drop(columns=['class'
] + cols_to_drop)
    
    # Handle Categorical
    # Check what columns remain
    print("Remaining columns:", X.columns)
    
    # Example cols: purchase_value, source, browser, sex, age, country, device_txn_count...
    
    cat_cols = X.select_dtypes(include=['object'
]).columns
    num_cols = X.select_dtypes(include=['number'
]).columns
    
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', StandardScaler(), num_cols),
            ('cat', OneHotEncoder(handle_unknown='ignore', sparse=False), cat_cols) 
            # sparse=False for easier display/debug, True for memory
]
    )
    
    X_processed = preprocessor.fit_transform(X)
    print("Transformed Shape:", X_processed.shape)

# 6. Handle Imbalance
if 'X_processed' in locals():
    print("Resampling...")
    print("Original Class Dist:", Counter(y))
    
    sm = SMOTE(random_state=42)
    X_res, y_res = sm.fit_resample(X_processed, y)
    
    print("Resampled Class Dist:", Counter(y_res))
    
    # Save processed data if needed
    # pd.DataFrame(X_res).to_csv('../data/processed/fraud_processed.csv', index=False)